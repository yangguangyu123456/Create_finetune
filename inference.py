#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek-R1-Distill-Qwen-1.5B ç…¤çŸ¿å®‰å…¨é—®ç­”æ¨ç†è„šæœ¬
æ”¯æŒåŠ è½½å¾®è°ƒåçš„LoRAæ¨¡å‹è¿›è¡Œæ¨ç†
"""

import os
import json
import argparse

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CoalSafetyInference:
    """ç…¤çŸ¿å®‰å…¨é—®ç­”æ¨ç†ç±»"""

    def __init__(self, base_model_path: str, peft_model_path: str = None, use_4bit: bool = True):
        self.base_model_path = base_model_path
        self.peft_model_path = peft_model_path
        self.use_4bit = use_4bit
        self.model = None
        self.tokenizer = None

        self.load_model()

    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.base_model_path}")

        # é…ç½®é‡åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.use_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
        else:
            bnb_config = None

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_path,
            trust_remote_code=True,
            padding_side="right"
        )

        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if not self.use_4bit else None
        )

        # å¦‚æœæœ‰LoRAæ¨¡å‹ï¼ŒåŠ è½½å®ƒ
        if self.peft_model_path and os.path.exists(self.peft_model_path):
            logger.info(f"åŠ è½½LoRAæ¨¡å‹: {self.peft_model_path}")
            self.model = PeftModel.from_pretrained(self.model, self.peft_model_path)
            logger.info("LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            logger.info("ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹ï¼ˆæœªåŠ è½½LoRAï¼‰")

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")

    def format_prompt(self, question: str) -> str:
        """æ ¼å¼åŒ–è¾“å…¥æç¤º"""
        return f"system: ä½ æ˜¯ä¸€ä¸ªç…¤çŸ¿å®‰å…¨é¢†åŸŸçš„çŸ¥è¯†è¾¾äººï¼Œä½ å¯¹ç›¸å…³ç…¤çŸ¿å®‰å…¨è§„ç« è§„ç¨‹åˆ¶åº¦ã€æŠ€æœ¯ç­‰æ–‡æ¡£éå¸¸ç†Ÿæ‚‰ã€‚è¯·ä½ ä¸“ä¸šæ­£ç¡®åœ°è§£ç­”ç”¨æˆ·æƒ³é—®çš„ç…¤çŸ¿å®‰å…¨ç›¸å…³é—®é¢˜ã€‚\nuser: {question}\nresponse: "

    def generate_response(self, question: str, max_length: int = 512,
                          temperature: float = 0.7, top_p: float = 0.9,
                          do_sample: bool = True) -> str:
        """ç”Ÿæˆå›ç­”"""
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_prompt(question)

        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        )

        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )

        # è§£ç è¾“å‡º
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–åŠ©æ‰‹å›ç­”éƒ¨åˆ†
        if "response:" in response:
            response = response.split("response:")[-1]

        return response.strip()

    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        print("\n=== ç…¤çŸ¿å®‰å…¨é—®ç­”ç³»ç»Ÿ ===")
        print("è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'clear' æ¸…å±")
        print("=" * 50)

        while True:
            try:
                question = input("\nğŸ¤” æ‚¨çš„é—®é¢˜: ").strip()

                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break

                if question.lower() in ['clear', 'æ¸…å±']:
                    os.system('cls' if os.name == 'nt' else 'clear')
                    continue

                if not question:
                    continue

                print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
                response = self.generate_response(question)
                print(f"\nğŸ’¡ å›ç­”: {response}")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")

    def batch_inference(self, questions: list) -> list:
        """æ‰¹é‡æ¨ç†"""
        results = []
        for i, question in enumerate(questions, 1):
            logger.info(f"å¤„ç†é—®é¢˜ {i}/{len(questions)}: {question[:50]}...")
            try:
                response = self.generate_response(question)
                results.append({
                    "question": question,
                    "answer": response
                })
            except Exception as e:
                logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
                results.append({
                    "question": question,
                    "answer": f"é”™è¯¯: {str(e)}"
                })
        return results


def main():
    parser = argparse.ArgumentParser(description="DeepSeekç…¤çŸ¿å®‰å…¨é—®ç­”æ¨ç†")
    parser.add_argument(
        "--base_model",
        type=str,
        default="./DeepSeek-R1-Distill-Qwen-1.5B",
        help="åŸºç¡€æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--peft_model",
        type=str,
        default=None,
        help="LoRAæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--use_4bit",
        action="store_true",
        default=True,
        help="æ˜¯å¦ä½¿ç”¨4bité‡åŒ–"
    )
    parser.add_argument(
        "--question",
        type=str,
        default=None,
        help="å•ä¸ªé—®é¢˜ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--questions_file",
        type=str,
        default=None,
        help="é—®é¢˜æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰"
    )
    parser.add_argument(
        "--output_file",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--max_length",
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="ç”Ÿæˆæ¸©åº¦"
    )

    args = parser.parse_args()

    # åˆå§‹åŒ–æ¨ç†å™¨
    inferencer = CoalSafetyInference(
        base_model_path=args.base_model,
        peft_model_path=args.peft_model,
        use_4bit=args.use_4bit
    )

    # å•ä¸ªé—®é¢˜æ¨¡å¼
    if args.question:
        print(f"é—®é¢˜: {args.question}")
        response = inferencer.generate_response(
            args.question,
            max_length=args.max_length,
            temperature=args.temperature
        )
        print(f"å›ç­”: {response}")

    # æ‰¹é‡é—®é¢˜æ¨¡å¼
    elif args.questions_file:
        if not os.path.exists(args.questions_file):
            print(f"é”™è¯¯: é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {args.questions_file}")
            return

        with open(args.questions_file, 'r', encoding='utf-8') as f:
            questions_data = json.load(f)

        # æ”¯æŒä¸åŒçš„JSONæ ¼å¼
        if isinstance(questions_data, list):
            if isinstance(questions_data[0], str):
                questions = questions_data
            else:
                questions = [item.get('question', item.get('input', '')) for item in questions_data]
        else:
            questions = questions_data.get('questions', [])

        results = inferencer.batch_inference(questions)

        # ä¿å­˜ç»“æœ
        if args.output_file:
            with open(args.output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
        else:
            for result in results:
                print(f"\né—®é¢˜: {result['question']}")
                print(f"å›ç­”: {result['answer']}")
                print("-" * 50)

    # äº¤äº’æ¨¡å¼
    else:
        inferencer.interactive_chat()


if __name__ == "__main__":
    main()